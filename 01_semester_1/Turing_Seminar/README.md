# Course: Turing Seminar — Safety and Interpretability of General-Purpose AI  
(*Séminaire Turing – Sûreté et interprétabilité des IA à usage général*)

### Course Context

- **Professor:** Charbel-Raphaël Segerie (ENS Paris-Saclay)  
- **Program:** Master MVA – Mathematics, Vision, Learning  
- **ECTS:** 2.5  
- **Grade:** 14.00 / 20  
- **Year:** 2024 – 2025  

The **Turing Seminar** focuses on the **philosophical, ethical, and safety aspects** of Artificial Intelligence, connecting mathematical and computational foundations with questions of responsibility, transparency, and long-term risk.  
It encourages interdisciplinary reasoning — blending insights from **machine learning**, **ethics**, **philosophy of mind**, and **governance studies** — to critically examine how AI systems interact with society.

The course culminates in an **essay or research review**, where students explore a specific issue related to AI safety, interpretability, or alignment.

---

### Final Dissertation — *What Is the Probability That AI Poses Catastrophic Risks Assuming There Is No Deceptive Alignment?*  

**Authors:** Thomas Gravier & Rosalie Millner  

---

### Abstract

This essay investigates the **likelihood of catastrophic AI risks** under the assumption that **no deceptive alignment** occurs — meaning AI systems act according to their designed objectives without intentionally hiding misaligned goals.  
The analysis explores whether **catastrophic outcomes** could still arise from systemic, structural, or socio-technical factors rather than from adversarial intent.

---

### 1. Conceptual Background  

AI safety discourse often emphasizes *deceptive alignment* — scenarios where systems appear aligned but secretly pursue harmful objectives.  
However, even in its absence, **AI could still pose existential or societal risks**, including:  

- **Malicious use** (biotech, cyberwarfare, disinformation),  
- **Competitive deployment pressures** (“AI arms race”),  
- **Organizational or technical mismanagement**,  
- **Misaligned incentives** between research and safety.

The paper distinguishes between *existential risks* (human extinction or irreversible collapse), *societal disruptions* (mass unemployment, instability), and *ethical risks* (loss of agency or value erosion).

---

### 2. Pathways to Catastrophic Outcomes Without Deception  

#### **Malicious Use**
Open-access AI models and rapid capability diffusion increase the probability of misuse by malicious actors.

#### **Acceleration Dynamics**
Competition among corporations and nations incentivizes premature deployment, leading to “race-to-the-bottom” safety standards.  

#### **Systemic Failures**
Non-deceptive AI may still act catastrophically due to:
- Over-optimization of proxy objectives,  
- Lack of robust oversight mechanisms,  
- Complex interactions between AI systems and critical infrastructure.

---

### 3. Mitigation Strategies  

The essay proposes three levels of mitigation:

1. **Governance and Regulation:**  
   International safety treaties, AI auditability standards, and global coordination mechanisms.  

2. **Research and Technical Safety:**  
   Improved interpretability, robustness, and safe scaling of AI systems.  

3. **Institutional Culture:**  
   Embedding safety and ethics within research labs; promoting transparency, verification, and accountability.  

---

### 4. Evaluating Risk Probability  

Quantifying catastrophic AI risk remains speculative.  
Drawing from epistemic uncertainty, historical analogies, and scenario modeling, the authors conclude that:

> Even in the absence of deception, AI carries a **non-negligible probability** of catastrophic failure — driven by sociotechnical factors rather than inherent malevolence.

This highlights the need to treat **AI safety as a systemic challenge**, not solely a technical one.

---

### Conclusion  

The dissertation argues that **catastrophic AI risks do not require deceptive alignment** — they can emerge organically from **institutional incentives, misuse, and lack of preparedness**.  
While the exact probability is incalculable, **preventive governance and robust safety practices** are essential to reduce it.

The essay emphasizes that true AI safety demands not only aligned objectives but also **responsible deployment ecosystems**.

---


# Course: Convex Optimization and Applications in Machine Learning

### Course Context

- **Professor:** Alexandre d’Aspremont (ENS, INRIA – DI ENS, CNRS, Inria, PSL University)  
- **Program:** Master MVA (Mathematics, Vision, Learning), ENS Paris-Saclay  
- **ECTS:** 5  
- **Grade:** 17.48 / 20  
- **Year:** 2024 – 2025  

This course provides both the theoretical foundations and practical algorithms for **convex optimization**, one of the mathematical pillars of modern machine learning, signal processing, and data science.  
It is taught by **Prof. Alexandre d’Aspremont**, one of the leading experts in optimization theory and its applications in large-scale learning problems.

---

### Course Objectives

The course aims to teach students how to:
- Identify and **formulate convex optimization problems** in various domains;  
- Understand the **geometry of convexity** and the theory of **duality**;  
- Implement and analyze **optimization algorithms** (first-order and second-order);  
- Apply convex optimization methods to **machine learning, sparse estimation, and signal processing**.

---

### Main Topics Covered

1. **Convex sets and convex functions**  
   - Convex cones, affine sets, subgradients, Jensen’s inequality.  
2. **Duality theory**  
   - Lagrangian duality, KKT conditions, Fenchel conjugate functions.  
3. **Algorithms for convex optimization**  
   - Gradient descent, subgradient methods, coordinate descent, accelerated methods (Nesterov).  
4. **Interior-point and barrier methods**  
   - Newton’s method, path-following, complexity analysis.  
5. **Applications in Machine Learning**  
   - Lasso, SVMs, logistic regression, matrix completion, and semidefinite programming (SDP).  
6. **Convex relaxations and large-scale optimization**  
   - Convex approximations of nonconvex problems, stochastic gradients, proximal operators.  

---

### My Work and Deliverables

The coursework included:
- **Mathematical assignments** with detailed derivations and proofs;  
- **Implementation of optimization algorithms** in Python and MATLAB;  
- **Mini-projects** applying convex optimization to machine learning tasks (e.g., sparse regression, classification, SDP relaxations).  

Each lab required both theoretical reasoning and empirical validation on real datasets.

---

### Insights and Key Takeaways

- Convexity provides **global optimality guarantees** — a unique and stable solution to optimization problems.  
- Duality principles enable elegant **theoretical analysis** and **efficient algorithms**.  
- Modern ML problems (regularization, kernel methods, deep learning optimization) are often built on **convex cores** or approximations.  
- Understanding **algorithmic trade-offs** between convergence rate and computational complexity is essential for practical optimization.  

---

### Why This Course Matters

Convex Optimization is a **core mathematical tool** underpinning most of modern machine learning:
- Support Vector Machines (SVMs), logistic regression, and Lasso are convex problems.  
- Many **non-convex ML models** (e.g., deep networks) are optimized using convex relaxations or first-order convex techniques.  
- The theory learned here is foundational for research in **optimization, learning theory, and applied mathematics**.

---

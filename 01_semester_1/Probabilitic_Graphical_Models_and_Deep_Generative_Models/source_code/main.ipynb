{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059caccf-0f2a-405e-96cf-7e2b92ac86e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e29f93",
   "metadata": {},
   "source": [
    "## **CONFIG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c60425c-4a7f-4b61-823e-c8ab69a445e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_preparation import transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "656f60cf-e3dd-4d44-9ab0-386505bdb61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "IMG_DIR = config['data']['data_dir']\n",
    "ANNOS_FILE = config['data']['annos_file']\n",
    "BATCH_SIZE = config['training']['batch_size']\n",
    "IMG_SIZE = (config['data']['image_size'], config['data']['image_size'])\n",
    "\n",
    "TRANSFORM = transform_data(IMG_SIZE)\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available else 'cpu'\n",
    "\n",
    "LR = config['training']['learning_rate']\n",
    "GAMMA = config['scheduler']['gamma']\n",
    "STEP_SIZE = config['scheduler']['step_size']\n",
    "\n",
    "DISCRIMINANT_CRITERION = nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_EPOCHS = config['training']['num_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870e128",
   "metadata": {},
   "source": [
    "## **PRE-PROP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41a481-8e64-4ea1-9e6a-5952c7a21ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_preparation import generate_json_annotations\n",
    "\n",
    "generate_json_annotations(IMG_DIR, ANNOS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2347ec-c306-42ce-8d95-4a0aaf81b404",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **DATASET \\& LOADERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2114672-bc38-409b-a489-1b8db9ae6809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_preparation import ImageDataset, train_val_dataset, train_val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f893b10e-0591-45df-b99d-6e5a1a131fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = ImageDataset(annotations_file='annotations_classif.json',\n",
    "                      img_dir=IMG_DIR,\n",
    "                      transform=TRANSFORM)\n",
    "\n",
    "TRAIN_TEST_SET = train_val_dataset(DATASET, 0.1)\n",
    "TRAIN_SET = TRAIN_TEST_SET['train']\n",
    "TRAIN_VAL_SET = train_val_dataset(TRAIN_SET, 0.2)\n",
    "TRAIN_VAL_LOADER = train_val_dataloader(TRAIN_VAL_SET, BATCH_SIZE)\n",
    "\n",
    "TEST_SET = TRAIN_TEST_SET['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f44bad-6e8a-4dba-af92-6bb4785be0c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a0fd03c-df53-4578-9550-be16ef51fef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from train import evalutrain_model\n",
    "from models import FineTunedResNet18, GBZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81ac1a",
   "metadata": {},
   "source": [
    "## **GBZ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7264504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_channels = 1\n",
    "output_channels = 1\n",
    "latent_dim = 100\n",
    "\n",
    "model = GBZ(latent_dim, num_classes=6).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b836b2-9d80-4757-9207-17f703fbe54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit_model, train_losses, train_accs, val_losses, val_accs = evalutrain_model(model,\n",
    "                'vae_gbz',\n",
    "                TRAIN_VAL_LOADER,\n",
    "                TRAIN_VAL_SET,\n",
    "                DISCRIMINANT_CRITERION,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                DEVICE,\n",
    "                num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac6e65-8859-4075-84f1-018e143f7137",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inf√©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507dfeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_acc_train\n",
    "plot_acc_train(train_losses,\n",
    "               train_accs,\n",
    "               val_losses,\n",
    "               val_accs,\n",
    "               type_model='vae_gbz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383715ff-3007-4da5-90e7-764069205c89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluation import test_evaluation\n",
    "\n",
    "true_labels, pred_labels, probs_positive, true_logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='vae_gbz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a414bc-2b57-4509-aa72-88923162e313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluation import plot_confusion_matrix\n",
    "\n",
    "cm = plot_confusion_matrix(true_labels, pred_labels, attack_name='vae_gbz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e89628",
   "metadata": {},
   "source": [
    "## **DISCRIMINANT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11c4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_channels = 1\n",
    "output_channels = 1\n",
    "latent_dim = 20\n",
    "\n",
    "# model = GBZ(input_channels, latent_dim, num_classes=6, output_channels=output_channels, image_size=68).to(DEVICE)\n",
    "model = FineTunedResNet18().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model, train_losses, train_accs, val_losses, val_accs = evalutrain_model(model,\n",
    "                'convnet',\n",
    "                TRAIN_VAL_LOADER,\n",
    "                TRAIN_VAL_SET,\n",
    "                DISCRIMINANT_CRITERION,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                DEVICE,\n",
    "                num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc228b27",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d134e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_acc_train\n",
    "plot_acc_train(train_losses,\n",
    "               train_accs,\n",
    "               val_losses,\n",
    "               val_accs,\n",
    "               type_model='convnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import test_evaluation\n",
    "\n",
    "true_labels, pred_labels, probs_positive, true_logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e053dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import plot_confusion_matrix\n",
    "\n",
    "cm = plot_confusion_matrix(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5a632",
   "metadata": {},
   "source": [
    "## **PERFORM ATTACKS**\n",
    "\n",
    "***\n",
    "\n",
    "When executing the below cells, please update `model_type` parameter to which model you are using, and which `attack` you want to perform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62902f94",
   "metadata": {},
   "source": [
    "### **CONTRAST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import perform_attack\n",
    "\n",
    "adv_images_contrast = perform_attack(fit_model, TEST_SET, model_type='convnet', attack='contrast', epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc17980",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels_attack_contrast, probs_positive, logits_convnet_contrast = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images_contrast\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe9a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "true_images = np.array([img for img, _ in TEST_SET])\n",
    "adv_images_bg = np.array([img.cpu().detach().numpy() for img in adv_images_contrast])\n",
    "i = 5678\n",
    "\n",
    "# plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(adv_images_bg[i].squeeze(), cmap='gray')\n",
    "plt.title('Adversarial Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_images[i].squeeze(), cmap='gray')\n",
    "plt.title('True Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(true_images[i].squeeze() - adv_images_bg[i].squeeze(), cmap='gray')\n",
    "plt.title('Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plot_confusion_matrix(true_labels, pred_labels_attack_contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01559ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_labels, pred_labels_attack_contrast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc26f3c",
   "metadata": {},
   "source": [
    "### **BRIGHTNESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096acbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import perform_attack\n",
    "\n",
    "epsilons = [0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "accuracy_scores_convnet = []\n",
    "adv_images_bright = []\n",
    "for eps in epsilons:\n",
    "    adv_images = perform_attack(fit_model, TEST_SET, model_type='convnet', attack='brightness', epsilon=eps)\n",
    "    true_labels, pred_labels_attack, probs_positive, logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images\n",
    "                                                            )\n",
    "    accuracy_scores_convnet.append(accuracy_score(true_labels, pred_labels_attack))\n",
    "    adv_images_bright.append(adv_images)\n",
    "    print(f'accuracy score with attack: {accuracy_score(true_labels, pred_labels_attack)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels_attack_bright, probs_positive, logits_convnet_br = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images_bright\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f168ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "true_images = np.array([img for img, _ in TEST_SET])\n",
    "adv_images_bg = np.array([img.cpu().detach().numpy() for img in adv_images_bright])\n",
    "i = 4000\n",
    "\n",
    "# plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(adv_images_bg[i].squeeze(), cmap='gray')\n",
    "plt.title('Adversarial Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(true_images[i].squeeze(), cmap='gray')\n",
    "plt.title('True Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(true_images[i].squeeze() - adv_images_bg[i].squeeze(), cmap='gray')\n",
    "plt.title('la grosse Difference')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aa643",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plot_confusion_matrix(true_labels, pred_labels_attack_bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(true_labels, pred_labels_attack_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9c3208",
   "metadata": {},
   "source": [
    "### **ONE PIXEL DISCRIMINANT**\n",
    "\n",
    "This part is to perform ONE PIXEL ATTACK on the discriminant ResNet18 Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe724054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks import run_one_pixel_attack\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "list_labels = []\n",
    "for name in os.listdir(IMG_DIR):\n",
    "    list_labels.append(name)\n",
    "list_labels = np.sort(list_labels)\n",
    "test_loader = torch.utils.data.DataLoader(TEST_SET, batch_size=1, shuffle=False)\n",
    "fit_model = fit_model.to(DEVICE)\n",
    "adv_images_pixel = run_one_pixel_attack(fit_model, test_loader, list_labels, model_type='convnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c895d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_images_pixel = torch.tensor(adv_images_pixel)\n",
    "true_labels, pred_labels_attack_pixel, probs_positive, logits_convnet_pixel = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images_pixel\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plot_confusion_matrix(true_labels, pred_labels_attack_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bed282",
   "metadata": {},
   "source": [
    "### **FSGM ATTACK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c88f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsgm attack\n",
    "from evaluation import perform_attack\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epsilons = [0.00,0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "accuracy_scores_convnet = []\n",
    "tot_adv_images = []\n",
    "for eps in epsilons:\n",
    "    adv_images = perform_attack(fit_model, TEST_SET, model_type='vae_gbz', attack='pgd', epsilon=eps)\n",
    "    true_labels, pred_labels_attack, probs_positive, logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='vae_gbz',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images\n",
    "                                                            )\n",
    "    accuracy_scores_convnet.append(accuracy_score(true_labels, pred_labels_attack))\n",
    "    tot_adv_images.append(adv_images)\n",
    "    print(f'accuracy score with attack: {accuracy_score(true_labels, pred_labels_attack)} \\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot images, where the big figure x axis depends on epsilon and y axis the different images\n",
    "fig, axs = plt.subplots(6, 6, figsize=(10, 10))\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axs[i, j].imshow(tot_adv_images[j][i].detach().numpy().squeeze(), cmap='gray')\n",
    "        if i == 0:\n",
    "            axs[i, j].set_title(f'epsilon: {epsilons[j]}', fontsize=8)\n",
    "        axs[i, j].axis('off')\n",
    "       \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.savefig('./Inference/pgd_attacks_test_images_vae_gbz.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = plot_confusion_matrix(true_labels, pred_labels_attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'accuracy score with attack: {accuracy_score(true_labels, pred_labels_attack)} \\n')\n",
    "print(f'accuracy score without attack: {accuracy_score(true_labels, pred_labels)} \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7af9a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "true_images = np.array([img for img, _ in TEST_SET])\n",
    "adv_images = np.array([img.cpu().detach().numpy() for img in adv_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d986d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "i = 4000\n",
    "\n",
    "# plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(adv_images[i].squeeze(), cmap='gray')\n",
    "plt.title('Adversarial Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(true_images[i].squeeze(), cmap='gray')\n",
    "plt.title('True Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41296600",
   "metadata": {},
   "source": [
    "### **ONE PIXEL ATTACK**\n",
    "\n",
    "This part is to perform ONE PIXEL ATTACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2716bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacks import run_one_pixel_attack\n",
    "\n",
    "\n",
    "list_labels = []\n",
    "for name in os.listdir(IMG_DIR):\n",
    "    list_labels.append(name)\n",
    "list_labels = np.sort(list_labels)\n",
    "test_loader = torch.utils.data.DataLoader(TEST_SET, batch_size=1, shuffle=False)\n",
    "fit_model = fit_model.to(DEVICE)\n",
    "adv_images_pixel = run_one_pixel_attack(fit_model, test_loader, list_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b5cc59f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images_pixel = np.array([img.cpu().detach().numpy() for img in adv_images_pixel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f98f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_img_pixel = np.array(adv_images_pixel).squeeze(1)\n",
    "i = 5\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(adv_img_pixel[i].squeeze(), cmap='gray')\n",
    "plt.title('Adversarial Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(true_images[i].squeeze(), cmap='gray')\n",
    "plt.title('True Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43157e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images_pixel = torch.tensor(adv_images_pixel).to('cpu')\n",
    "true_labels, pred_labels_attack_pixel, probs_positive, logits = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='vae_gbz',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images_pixel\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0be1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_pixel = torch.max(torch.Tensor(logits).squeeze(), dim=1)\n",
    "true_logits = torch.max(torch.Tensor(true_logs).squeeze(), dim=1)\n",
    "\n",
    "print(f'error rate with pixel attack: {np.array(logits_pixel) / np.array(true_logits) / 100} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6103a8",
   "metadata": {},
   "source": [
    "### **PGD ATTACK**\n",
    "\n",
    "This part is to perform PGD ATTACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images = perform_attack(fit_model, TEST_SET, model_type='vae_gbz', attack='pgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels_pgd, probs_positive, logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='vae_gbz',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy score with attack: {accuracy_score(true_labels, pred_labels_pgd)} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f54c3",
   "metadata": {},
   "source": [
    "### **CW ATTACK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3198cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images = perform_attack(fit_model, TEST_SET, model_type='convet', attack='cw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21920b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels_cw, probs_positive, logs = test_evaluation(fit_model, \n",
    "                                                            TEST_SET,\n",
    "                                                            device='cpu',\n",
    "                                                            type_model='convnet',\n",
    "                                                            attack=True,\n",
    "                                                            adv_images=adv_images\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy score with attack: {accuracy_score(true_labels, pred_labels_cw)} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf5521",
   "metadata": {},
   "source": [
    "## **PLOT LATENT SPACE WITH PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8af89261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in TEST_SET:\n",
    "    z = fit_model.get_z(x.unsqueeze(0).to(DEVICE))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd4bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_latent_space\n",
    "\n",
    "plot_latent_space(fit_model, TEST_SET, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d226cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [0.998, 0.98, 0.94, 0.88, 0.81, 0.78]\n",
    "list2 = [0.999, 0.84, 0.48, 0.41, 0.36, 0.33]\n",
    "epsilons = [0, 0.01, 0.03, 0.05, 0.08, 0.1]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(epsilons, list1, \"-\", label='gbz', color='mediumslateblue')\n",
    "plt.plot(epsilons, list2, \"-\", label='resnet18', color='mediumvioletred')\n",
    "plt.scatter(epsilons, list1, marker='*', color='mediumslateblue', s=100)\n",
    "plt.scatter(epsilons, list2, marker='*', color='mediumvioletred', s=100)\n",
    "plt.yticks(np.arange(0.3, 1.1, step=0.2))\n",
    "plt.grid(True)\n",
    "plt.xlabel(r\"$\\epsilon \\in \\{0.01, 0.03, 0.05, 0.08, 0.1\\}$\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('./Inference/accs_models_fsgm.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pgm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Course: Probabilistic Graphical Models (PGM)

### Course Context

- **Professor:** Pierre Latouche (Université Paris Cité, CNRS, MAP5)  
- **Program:** Master MVA – Mathematics, Vision, Learning (ENS Paris-Saclay)  
- **ECTS:** 5  
- **Grade:** 17.50 / 20  
- **Year:** 2024 – 2025  

This course provides theoretical and practical foundations for **probabilistic graphical models**, **inference methods**, and **deep generative modeling**.  
It bridges the gap between **classical probabilistic modeling** (Bayesian networks, Markov Random Fields) and **modern deep learning approaches** (VAEs, normalizing flows, diffusion models).  

The course emphasizes both **mathematical understanding** and **hands-on implementation** of probabilistic models for structured and high-dimensional data.

---

### Project Overview

**Title:** *Are Generative Classifiers More Reliable for Medical Imaging? Insights from Adversarial and Non-Adversarial Perturbations*  
**Authors:** Rosalie Millner, Emilio Picard, Thomas Gravier  
**Institution:** ENS Paris-Saclay — Master MVA  

Our team conducted an experimental and theoretical study inspired by *Y. Li, J. Bradshaw, and Y. Sharma (2019)*, focusing on the **robustness of probabilistic generative classifiers** compared to **classical discriminative models** in the context of **medical imaging**.

The goal was to evaluate how **probabilistic graphical models**—which explicitly model the joint data distribution \( p(x, y) \)—handle perturbations compared to discriminative neural networks that only model \( p(y|x) \).

---

### Motivation

With the increasing volume of medical images generated by various radiological imaging techniques, the use of AI assistance can significantly enhance clinical applications. However, even a minimal error, such as a one-pixel discrepancy in an image, or a change in brightness or contrast, can lead to incorrect predictions in medical image analysis. Such errors could result in misclassifications, which might lead to wrong clinical decisions. This vulnerability can be seen as adversarial/non-adversarial attacks on deep learning models. In this report, we explore the extent to which a deep generative classifier and a deep discriminative classifier are robust to perturbations, using a widely used MRI image multiclass dataset. Furthermore, experiments explore how altering medical images influences classification accuracy and the robustness of different DNN/GNN architectures. Results demonstrate that discriminative models for medical images are susceptible to these attacks, whereas GNN can be much more robust and thus lead to less class prediction errors.



Modern medical imaging pipelines rely heavily on deep learning models for diagnosis.  
However, even minimal changes (a single pixel, brightness, or contrast shifts) can cause severe misclassifications.  
Since such errors can have critical implications in healthcare, improving the **robustness and reliability** of models is essential.

We hypothesized that **probabilistic generative models**, due to their explicit handling of uncertainty, would be more stable and resistant to both **adversarial** and **non-adversarial** perturbations.

---

### Methodology

We implemented and compared:

- A **Generative Bayesian classifier (GBZ model)** based on a **Variational Autoencoder (VAE)**  
- A **Discriminative classifier (DBX model)** using a **ResNet-18** backbone  

#### Attacks Studied
- **Adversarial attacks:**
  - Fast Gradient Sign Method (FGSM)
  - Projected Gradient Descent (PGD)
  - One-Pixel Attack  
- **Non-adversarial (natural) perturbations:**
  - Brightness variations  
  - Contrast variations  

#### Dataset
- **Medical MNIST**, composed of 50,000 MRI and CT images across six classes.  
- Models trained and evaluated using 5-fold cross-validation on an NVIDIA RTX 4050 GPU.

---

### Results Summary

| Attack Type | ResNet18 Accuracy | GBZ (Generative) Accuracy |
|--------------|------------------:|--------------------------:|
| Clean Data | 0.999 | 0.998 |
| FGSM (ε=0.03) | 0.82 | **0.91** |
| FGSM (ε=0.1) | 0.68 | **0.78** |
| PGD (ε=0.1) | 0.61 | **0.88** |
| Brightness Perturbation | 0.83 | 0.83 |
| Contrast Perturbation | 0.73 | **0.88** |

➡️ **Key finding:** Probabilistic (generative) models show consistently higher robustness under both adversarial and natural perturbations compared to standard discriminative classifiers.

---

### Theoretical Contributions

- Implementation of **VAE-based probabilistic classifiers** trained with an ELBO loss combining reconstruction, KL divergence, and classification terms.  
- Visualization of the **latent variable structure** using PCA, confirming that generative models form clearer, more separable clusters in latent space.  
- Empirical validation that **modeling uncertainty explicitly** leads to more stable and interpretable behavior under small perturbations.

---

### Conclusion

Our experiments demonstrate that **probabilistic generative classifiers are significantly more robust** than classical discriminative ones when dealing with both adversarial and natural perturbations in medical imaging.  

By explicitly modeling the data distribution and uncertainty, **probabilistic graphical models** provide a **stronger defense** against noise, attacks, and data shifts.  
This makes them a **safer and more reliable framework** for critical real-world applications such as healthcare, where robustness directly impacts decision quality.

---

### Structure of the Repository



**Please use `requirements.txt` or either `environment.yml` to run the provided code.

**Please download Medical MNIST on https://www.kaggle.com/datasets/andrewmvd/medical-mnist and extract it on Data

